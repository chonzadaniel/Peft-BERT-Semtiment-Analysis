{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec49b9d1-fdd7-4bd2-93b9-c27550fd6ed5",
   "metadata": {},
   "source": [
    "# Parameter-Efficient Fine-Tuning of BERT for Text Classification using QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26d775-414d-489e-b80c-67a28021795d",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "\n",
    "The main objective of this project is to apply parameter-efficient fine-tuning (PEFT) techniques, speficially QLoRA (Quantized Low-Rank Adaptation), to fine-tune a `BERT-base language model for a text classification task` while significantly reducing the computational and memory requrements typically associated with fine-tuning large language models (LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbbb763-e279-4bef-a69e-63ec83a85ea0",
   "metadata": {},
   "source": [
    "## Background and Motivation:\n",
    "\n",
    "Large Language Models (LLMs) like BERT have revolutionalized NLP tasks such as sentiment analysis, question answering, and classification tasks. Howeverr, full fine-tuning of such models is computationally expensive and requires vast resources. To Ensure fine-tuning feasible for resource-contrained environments (e.g., personal machines or small servers), researchers have developed PEFT methods like LoRA and its optimized variant QLoRA.\n",
    "\n",
    "QLoRA uses quantization (e.g., 4-bit quantization) and low-rank adaptations to fine-tune only a small portion of the model, reducing both memory and time costs without significatly compromising accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2861adcd-f210-4cb0-b1ae-6d783766c4e5",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "\n",
    "1. Data Preparation and Tokenizatiion:\n",
    "   * Load a binary classification dataset: [From HuggingFace](https://huggingface.co/datasets/dipanjanS/imdb_sentiment_finetune_dataset20k) and [KaggleHub](https://www.kaggle.com/datasets/bhavikjikadara/imdb-dataset-sentiment-analysis)\n",
    "   * Preprocess the data: tokenize the input text using a tokenizer compatible with BERT or HuggingFace models.\n",
    "   * Split dataset into train and test sets.\n",
    "2. Model Setup:\n",
    "   * Load a pre-trained bert-base-uncased model using HuggingFace Transformers.\n",
    "   * Apply 4-bit quantization uising bitsandbytes.\n",
    "   * Integrate QLoRA adapters with LoRA configuration.\n",
    "3. Training the Model:\n",
    "   * Configure a Trainer using HuggingFace's transformers.Trainer API\n",
    "   * Train the model using parameter-efficient stratagies.\n",
    "   * Monitor evaluation metrics such as loss and accuracy. Logging training to Weights and biases.\n",
    "4. Evaluation:\n",
    "   * Evaluate the fine-tuned model on the test set\n",
    "   * Compare peformance in terms of classification accuracy and memory/parameter efficiency.\n",
    "5. Analysis and Interpretation:\n",
    "   * Analyze the number of trainable parameters before and after applying QLoRA.\n",
    "   * Determine the Memory efficiency gained by using QLoRA instead of full fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822f7e6e-eca3-4cc1-8553-71c82c41e6f5",
   "metadata": {},
   "source": [
    "# QLoRA fine-tuning of a BERT SLM for Classification\n",
    "\n",
    "![](https://i.imgur.com/2Kw1yTZ.gif)\n",
    "\n",
    "Transfer Learning is the power of leveraging already trained models and tune \\ adapt them to our own downstream tasks. \n",
    "\n",
    "Digs up how to fine-tune a simple BERT Small Language Model (SLM) step by step for a simple yet essential task in NLP - Text Classification for Sentiment Analysis \n",
    "\n",
    "Instead of full-finetuning, PEFT methodologies used, more notably the Quantized Low-Rank Adaptation (QLoRA) technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe478040-7638-488b-b53c-bacd483d67f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.8.0 in /opt/anaconda3/lib/python3.12/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (2025.7.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.8.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch==2.8.0) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3149cbab-8632-4373-9160-358c26adb46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15a41c2-3ab1-48cf-9064-a2990a79280b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "# Check for Compute power (CUDA)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c27ccd3-cf3e-46e9-8bff-c171a5268968",
   "metadata": {},
   "source": [
    "# 1. HuggingFace Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4912d72-c472-456e-8c2f-dd970b7fa394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------- HuggingFace API Environment Setup ----------------------------------------------------\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = open('HUGGINGFACE_API_TOKEN.txt','r').read()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec100cbd-53ad-410f-8ace-1b344f83f8ce",
   "metadata": {},
   "source": [
    "## 2. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec24e53-8c9a-4652-9b90-300a64032610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Load Dataset\n",
    "dataset = pd.read_csv('/Users/emmanueldanielchonza/Documents/Parameter-Efficient-Fine-tuning-LLMs/data/IMDB_dataset.csv', encoding='utf-8')\n",
    "\n",
    "# View the first 5 rows\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da972471-42fe-4789-9781-1e7f938cef33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the dataset shape\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa5e44-aa91-4285-9e3b-75e06d7a24a3",
   "metadata": {},
   "source": [
    "### Get manageable data size for computation efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81252ea8-a8d7-44c1-9452-11db5fca3fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Sample of 20000 values\n",
    "data_sample = dataset.sample(frac=0.4)\n",
    "data_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1029b92-2477-4393-9795-48ab11de4bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Train set shape: (12000, 2)\n",
      "The test Set Shape is: (8000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Train and Test Data Split\n",
    "train_df = data_sample[:12000]\n",
    "test_df = data_sample[12000:]\n",
    "\n",
    "# Print Dataset shapes\n",
    "print(f\"The Train set shape: {train_df.shape}\")\n",
    "print(f\"The test Set Shape is: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "513d4a49-eeeb-4e59-a2a0-5b08fc338f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30379</th>\n",
       "      <td>I had numerous problems with this film.&lt;br /&gt;&lt;...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>This film was pretty good. I am not too big a ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11668</th>\n",
       "      <td>I remember when this came out a lot of kids we...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9179</th>\n",
       "      <td>I saw this movie in the theater when I was a k...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21458</th>\n",
       "      <td>I absolutely love Promised Land. The first epi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "30379  I had numerous problems with this film.<br /><...  negative\n",
       "381    This film was pretty good. I am not too big a ...  positive\n",
       "11668  I remember when this came out a lot of kids we...  negative\n",
       "9179   I saw this movie in the theater when I was a k...  negative\n",
       "21458  I absolutely love Promised Land. The first epi...  positive"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f89cfbaf-f860-4557-9c6b-e1eee20fc4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32706</th>\n",
       "      <td>For me an unsatisfactory, unconvincing heist m...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36025</th>\n",
       "      <td>Without a doubt one of the worst movies I've s...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25702</th>\n",
       "      <td>Saw this movie when it came out and then a cou...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>I don't have much to say about this movie. It ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33090</th>\n",
       "      <td>I'd never seen an independent movie and I was ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "32706  For me an unsatisfactory, unconvincing heist m...  negative\n",
       "36025  Without a doubt one of the worst movies I've s...  negative\n",
       "25702  Saw this movie when it came out and then a cou...  positive\n",
       "1737   I don't have much to say about this movie. It ...  negative\n",
       "33090  I'd never seen an independent movie and I was ...  positive"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ab934b-cc2a-4814-8a25-d91d5426e084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0.0\n",
       "sentiment    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in the train set\n",
    "train_df.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dee3c9e9-8b5e-4ddb-b630-7ab832069390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0.0\n",
       "sentiment    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in the test set\n",
    "test_df.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb08f2ff-51db-4336-b5ae-c53256ef5d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 12000 entries, 30379 to 36178\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     12000 non-null  object\n",
      " 1   sentiment  12000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 281.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check for datatype\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e982beb1-2c57-4481-8ab9-ab19ae8b24e9",
   "metadata": {},
   "source": [
    "##### Visible that columns have object datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee447aa-7c72-416c-9f70-765e1791c355",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e79079f9-cd98-4eb8-9fce-654011c2a15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dr/jc20rydd6t16ynt0fh_60l5w0000gn/T/ipykernel_26671/1281233664.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['label'] = train_df['sentiment'].map(LABEL2ID)\n",
      "/var/folders/dr/jc20rydd6t16ynt0fh_60l5w0000gn/T/ipykernel_26671/1281233664.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['label'] = test_df['sentiment'].map(LABEL2ID)\n"
     ]
    }
   ],
   "source": [
    "# Map LABEL2ID and ID2LABEL\n",
    "\n",
    "LABEL2ID = {'positive': 1, 'negative': 0}\n",
    "ID2LABEL = {1: 'positive', 0: 'negative'}\n",
    "\n",
    "# Create the label column\n",
    "train_df['label'] = train_df['sentiment'].map(LABEL2ID)\n",
    "test_df['label'] = test_df['sentiment'].map(LABEL2ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca864f68-baa6-4a9e-a689-76559c38c102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30379</th>\n",
       "      <td>I had numerous problems with this film.&lt;br /&gt;&lt;...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>This film was pretty good. I am not too big a ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11668</th>\n",
       "      <td>I remember when this came out a lot of kids we...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9179</th>\n",
       "      <td>I saw this movie in the theater when I was a k...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21458</th>\n",
       "      <td>I absolutely love Promised Land. The first epi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>A young couple -- father Ben (solid Charles Ba...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10967</th>\n",
       "      <td>STRANGER THAN FICTION angered me so much, I si...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21935</th>\n",
       "      <td>I decided I need to lengthen up my review for ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13933</th>\n",
       "      <td>Directed by Michael Curtiz, Four Daughters is ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6900</th>\n",
       "      <td>Dolemite may not have been the first black exp...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29966</th>\n",
       "      <td>While I recently gave OPERATION PETTICOAT a po...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37032</th>\n",
       "      <td>I saw this last night at a screening for a mar...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21125</th>\n",
       "      <td>I haven't seen this in over 20yrs but I still ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7571</th>\n",
       "      <td>I was living Rawlins when this movie was made ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12840</th>\n",
       "      <td>I think it's time John Rambo move on with his ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41859</th>\n",
       "      <td>The idea behind Dead Silence is great: zombie ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26677</th>\n",
       "      <td>I challenge you to watch this film and deny th...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36329</th>\n",
       "      <td>Hollywood's misguided obsession with sequels h...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17191</th>\n",
       "      <td>I saw this on cable recently and kinda enjoyed...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17852</th>\n",
       "      <td>higher learning is a slap in the face for thos...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18679</th>\n",
       "      <td>Even if you know absolutely nothing about Irel...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38309</th>\n",
       "      <td>'Mojo' is a story of fifties London, a world o...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39218</th>\n",
       "      <td>Definitely a very good idea,screenplay was jus...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545</th>\n",
       "      <td>Oh yes! Hollywood does remember how to use the...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Taut and organically gripping, Edward Dmytryk'...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  label\n",
       "30379  I had numerous problems with this film.<br /><...  negative      0\n",
       "381    This film was pretty good. I am not too big a ...  positive      1\n",
       "11668  I remember when this came out a lot of kids we...  negative      0\n",
       "9179   I saw this movie in the theater when I was a k...  negative      0\n",
       "21458  I absolutely love Promised Land. The first epi...  positive      1\n",
       "1887   A young couple -- father Ben (solid Charles Ba...  positive      1\n",
       "10967  STRANGER THAN FICTION angered me so much, I si...  negative      0\n",
       "21935  I decided I need to lengthen up my review for ...  positive      1\n",
       "13933  Directed by Michael Curtiz, Four Daughters is ...  positive      1\n",
       "6900   Dolemite may not have been the first black exp...  positive      1\n",
       "29966  While I recently gave OPERATION PETTICOAT a po...  negative      0\n",
       "37032  I saw this last night at a screening for a mar...  positive      1\n",
       "21125  I haven't seen this in over 20yrs but I still ...  positive      1\n",
       "7571   I was living Rawlins when this movie was made ...  positive      1\n",
       "12840  I think it's time John Rambo move on with his ...  negative      0\n",
       "41859  The idea behind Dead Silence is great: zombie ...  negative      0\n",
       "26677  I challenge you to watch this film and deny th...  negative      0\n",
       "36329  Hollywood's misguided obsession with sequels h...  negative      0\n",
       "17191  I saw this on cable recently and kinda enjoyed...  negative      0\n",
       "17852  higher learning is a slap in the face for thos...  positive      1\n",
       "18679  Even if you know absolutely nothing about Irel...  positive      1\n",
       "38309  'Mojo' is a story of fifties London, a world o...  negative      0\n",
       "39218  Definitely a very good idea,screenplay was jus...  positive      1\n",
       "2545   Oh yes! Hollywood does remember how to use the...  positive      1\n",
       "30     Taut and organically gripping, Edward Dmytryk'...  positive      1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the train set with a new column\n",
    "train_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "922d0b56-6659-4732-b95c-c0e4cfd6d641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32706</th>\n",
       "      <td>For me an unsatisfactory, unconvincing heist m...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36025</th>\n",
       "      <td>Without a doubt one of the worst movies I've s...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25702</th>\n",
       "      <td>Saw this movie when it came out and then a cou...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>I don't have much to say about this movie. It ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33090</th>\n",
       "      <td>I'd never seen an independent movie and I was ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6474</th>\n",
       "      <td>Directed by the younger brother of great direc...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12111</th>\n",
       "      <td>I generally love this type of movie. However, ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26517</th>\n",
       "      <td>After watching many of the \"Next Action Star\" ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9763</th>\n",
       "      <td>The Mascot is Ladislaw Starewicz's masterpiece...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>I loved Long Way Round and wasn't even aware o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14032</th>\n",
       "      <td>This movie is bad. I saw the rated and the unr...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44873</th>\n",
       "      <td>So why does this show suck? Unfortunately, tha...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13798</th>\n",
       "      <td>Although others have commented that this video...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4733</th>\n",
       "      <td>Without being really the worst science fiction...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6449</th>\n",
       "      <td>I wasn't going to watch this show. But, I'm gl...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34357</th>\n",
       "      <td>I am fifteen years old and have seen thirty-th...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32446</th>\n",
       "      <td>This completely forgotten slasher flick is one...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41772</th>\n",
       "      <td>This movie just seemed to lack direction. The ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45010</th>\n",
       "      <td>Anyone who had never seen anything like the fi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21370</th>\n",
       "      <td>A Great show.&lt;br /&gt;&lt;br /&gt;First, to the people ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>A response to previous comments made by reside...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45651</th>\n",
       "      <td>Even with the low standards of a dedicated hor...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16886</th>\n",
       "      <td>\"Sweeney Todd\" is in my opinion one of a few \"...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43846</th>\n",
       "      <td>This was not a well done western. You've got t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27518</th>\n",
       "      <td>If you like the standard Sly flicks that invol...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  label\n",
       "32706  For me an unsatisfactory, unconvincing heist m...  negative      0\n",
       "36025  Without a doubt one of the worst movies I've s...  negative      0\n",
       "25702  Saw this movie when it came out and then a cou...  positive      1\n",
       "1737   I don't have much to say about this movie. It ...  negative      0\n",
       "33090  I'd never seen an independent movie and I was ...  positive      1\n",
       "6474   Directed by the younger brother of great direc...  positive      1\n",
       "12111  I generally love this type of movie. However, ...  negative      0\n",
       "26517  After watching many of the \"Next Action Star\" ...  negative      0\n",
       "9763   The Mascot is Ladislaw Starewicz's masterpiece...  positive      1\n",
       "2054   I loved Long Way Round and wasn't even aware o...  positive      1\n",
       "14032  This movie is bad. I saw the rated and the unr...  negative      0\n",
       "44873  So why does this show suck? Unfortunately, tha...  negative      0\n",
       "13798  Although others have commented that this video...  positive      1\n",
       "4733   Without being really the worst science fiction...  negative      0\n",
       "6449   I wasn't going to watch this show. But, I'm gl...  positive      1\n",
       "34357  I am fifteen years old and have seen thirty-th...  positive      1\n",
       "32446  This completely forgotten slasher flick is one...  positive      1\n",
       "41772  This movie just seemed to lack direction. The ...  negative      0\n",
       "45010  Anyone who had never seen anything like the fi...  positive      1\n",
       "21370  A Great show.<br /><br />First, to the people ...  positive      1\n",
       "550    A response to previous comments made by reside...  negative      0\n",
       "45651  Even with the low standards of a dedicated hor...  negative      0\n",
       "16886  \"Sweeney Todd\" is in my opinion one of a few \"...  positive      1\n",
       "43846  This was not a well done western. You've got t...  negative      0\n",
       "27518  If you like the standard Sly flicks that invol...  negative      0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3576498f-17c5-43c8-9371-3f839178311b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    6053\n",
       "negative    5947\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the data distribution in the target column\n",
    "train_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9b9333d-dc2c-4e87-ab05-7f07a64dff1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    4049\n",
       "negative    3951\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the data distribution in the target column\n",
    "test_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c81e9-b8f1-4330-a3fe-fc2727e82307",
   "metadata": {},
   "source": [
    "Dataset nearly perfectly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1e33ce3-5a18-4007-be12-50aec6aa5c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30379</th>\n",
       "      <td>I had numerous problems with this film.&lt;br /&gt;&lt;...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>This film was pretty good. I am not too big a ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11668</th>\n",
       "      <td>I remember when this came out a lot of kids we...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9179</th>\n",
       "      <td>I saw this movie in the theater when I was a k...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21458</th>\n",
       "      <td>I absolutely love Promised Land. The first epi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41673</th>\n",
       "      <td>On one level, this film can bring out the chil...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28736</th>\n",
       "      <td>Oh mY God That has got to be one of the Most U...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>\"Quitting\" may be as much about exiting a pre-...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10027</th>\n",
       "      <td>I have just seen this movie and have not read ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36178</th>\n",
       "      <td>If you delete the first twenty minutes or so o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  label\n",
       "30379  I had numerous problems with this film.<br /><...  negative      0\n",
       "381    This film was pretty good. I am not too big a ...  positive      1\n",
       "11668  I remember when this came out a lot of kids we...  negative      0\n",
       "9179   I saw this movie in the theater when I was a k...  negative      0\n",
       "21458  I absolutely love Promised Land. The first epi...  positive      1\n",
       "...                                                  ...       ...    ...\n",
       "41673  On one level, this film can bring out the chil...  positive      1\n",
       "28736  Oh mY God That has got to be one of the Most U...  negative      0\n",
       "320    \"Quitting\" may be as much about exiting a pre-...  positive      1\n",
       "10027  I have just seen this movie and have not read ...  negative      0\n",
       "36178  If you delete the first twenty minutes or so o...  positive      1\n",
       "\n",
       "[12000 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save train and test split datasets\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7b5505-b7ae-4240-9bd6-329cad0361df",
   "metadata": {},
   "source": [
    "# Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1d17d41-e1ce-4e17-91ed-5fc89d677a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import logging\n",
    "\n",
    "# Get the model and instantiate tokenizer\n",
    "model_checkpoint = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "logging.set_verbosity_error()  # suppress info/progress messages\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10c60a07-e49c-403e-8c28-11b670b33ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google-bert/bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce7b5e-d40f-43ca-8227-fea59efdb940",
   "metadata": {},
   "source": [
    "# Convert Data to HuggingFace Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59c4f20d-c066-4763-9230-1a558cd10287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df[['review', 'label']], preserve_index=False),\n",
    "    'test': Dataset.from_pandas(test_df[['review', 'label']], preserve_index=False),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b9d3e17-139b-4606-84d7-50db48fe6e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'label'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'label'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3863ca96-da1b-4f84-87dd-70b7b023e698",
   "metadata": {},
   "source": [
    "## Save HuggingFace Formatted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09584aaf-dfeb-446d-8dc1-ef08c7222d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604d6c497ec5449da3892199e9c1871e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81c79c687364bc3b313b37929f6e543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Save the dataset locally\n",
    "dataset.save_to_disk(\"/Users/emmanueldanielchonza/Documents/Parameter-Efficient-Fine-tuning-LLMs/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d007cb7-7176-4039-91da-67626a2463fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['review', 'label'],\n",
      "        num_rows: 12000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['review', 'label'],\n",
      "        num_rows: 8000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"/Users/emmanueldanielchonza/Documents/Parameter-Efficient-Fine-tuning-LLMs/data\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "254d18bd-6977-4572-8d89-0d5b8045acde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the data split\n",
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9b798a7-f18e-4a41-b233-a0fe6b89fee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': ['I had numerous problems with this film.<br /><br />It contains some basic factual information concerning quantum mechanics, which is fine. Although quantum physics has been around for over 50 years, the film presents this information in a grandiose way that seems to be saying: \"Aren\\'t you just blown away by this!\" Well, not really. These aren\\'t earth shattering revelations anymore. At any rate, I was already familiar with quantum theory, and the fact that particles have to be described by wave equations, etc. is not new.<br /><br />The main problem I have with this movie, however, is the way these people use quantum theory as a way of providing a scientific basis for mysticism and spiritualism. I don\\'t have any serious problem with mysticism and spiritualism, but quantum mechanics doesn\\'t really have anything to do with these things, and it should be kept separate. The people they interviewed for this movie start with the ideas of quantum theory and then make the leap to say that simply by thinking about something you can alter the matter around you, hence we should think positively so as to have a positive impact on the world and make our lives better. The reasoning is completely ridiculous, and the conclusions do not logically follow from quantum theory. For every so called \"expert\" that they interviewed for this film, there are scores of theoretically physicists who would completely disagree. They would point out, quite rightly, that the unpredictability of the subatomic world does not lend support to mystical notions about our spiritual connectedness.<br /><br />It disturbs me that people are going to see this film and completely eat it up because it leaves them with a nice positive feeling. The main thrust of the film is based on a total misinterpretation of quantum theory, and it is as bad in its reasoning as any attempt to justify organized religion with similar pseudo-scientific arguments.<br /><br />Avoid this film.<br /><br />Oh yeah. At one point, one of the \"experts\" says that since throughout history most of the assumptions people have made about the world turned out to be false, therefore the assumptions we currently hold about the world are also likely to be false. Huh? That totally does not follow. And even if it did, I don\\'t see how that helps his argument. I mean, if his ideas ever became common assumptions then I guess we would have to assume that they are false too, based on his own reasoning.'],\n",
       " 'label': [0]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the first two rows of the train dataset\n",
    "dataset['train'][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747e2fd6-aaa4-4cc7-a233-4f35b8cf9510",
   "metadata": {},
   "source": [
    "# Create the Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab44635f-0272-4ddd-a463-e1af786349e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Key Classification Task Metrics\n",
    "metric1 = evaluate.load(\"precision\")\n",
    "metric2 = evaluate.load(\"recall\")\n",
    "metric3 = evaluate.load(\"f1\")\n",
    "metric4 = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Create a function to compute metrics\n",
    "def evaluate_performance(predictions, references):\n",
    "    precision = metric1.compute(predictions=predictions, references=references, average=\"macro\")[\"precision\"]\n",
    "    recall = metric2.compute(predictions=predictions, references=references, average=\"macro\")[\"recall\"]\n",
    "    f1 = metric3.compute(predictions=predictions, references=references, average=\"macro\")[\"f1\"]\n",
    "    accuracy = metric4.compute(predictions=predictions, references=references)[\"accuracy\"]\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6da1baab-c3a9-4f2e-a133-3a44a272b176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.5833333333333333,\n",
       " 'recall': 0.5833333333333333,\n",
       " 'f1': 0.5833333333333333,\n",
       " 'accuracy': 0.6}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "preds = [1,0,1,1,0]\n",
    "actuals = [1,1,0,1,0]\n",
    "scores = evaluate_performance(\n",
    "    predictions=preds, references=actuals\n",
    ")\n",
    "\n",
    "# View scores\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca2a00-6998-45fe-9e24-59c13bf217f4",
   "metadata": {},
   "source": [
    "# Create Tokens of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7336adde-a32f-4fa2-90e6-72d588b8b0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2023, 2003, 1037, 6251, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the tokenization functionality\n",
    "tokenizer(\"Hello, this is a sentence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b6811ac-f18a-4440-a259-24dd7c233450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to tokenize dataset\n",
    "def preprocess_function(examples):\n",
    "    # max length is 512 as that is the context window limit of BERT models\n",
    "    # It can process documents of upto 512 tokens each input\n",
    "    model_inputs = tokenizer(examples['review'], max_length=512, truncation=True)\n",
    "    model_inputs[\"label\"] = examples[\"label\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6583340c-be39-4e6c-a72d-09212918ecf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 2018, 3365, 3471, 2007, 2023, 2143, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2009, 3397, 2070, 3937, 25854, 2592, 7175, 8559, 9760, 1010, 2029, 2003, 2986, 1012, 2348, 8559, 5584, 2038, 2042, 2105, 2005, 2058, 2753, 2086, 1010, 1996, 2143, 7534, 2023, 2592, 1999, 1037, 2882, 10735, 2063, 2126, 2008, 3849, 2000, 2022, 3038, 1024, 1000, 4995, 1005, 1056, 2017, 2074, 10676, 2185, 2011, 2023, 999, 1000, 2092, 1010, 2025, 2428, 1012, 2122, 4995, 1005, 1056, 3011, 21797, 22191, 4902, 1012, 2012, 2151, 3446, 1010, 1045, 2001, 2525, 5220, 2007, 8559, 3399, 1010, 1998, 1996, 2755, 2008, 9309, 2031, 2000, 2022, 2649, 2011, 4400, 11380, 1010, 4385, 1012, 2003, 2025, 2047, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2364, 3291, 1045, 2031, 2007, 2023, 3185, 1010, 2174, 1010, 2003, 1996, 2126, 2122, 2111, 2224, 8559, 3399, 2004, 1037, 2126, 1997, 4346, 1037, 4045, 3978, 2005, 17477, 2964, 1998, 6259, 2964, 1012, 1045, 2123, 1005, 1056, 2031, 2151, 3809, 3291, 2007, 17477, 2964, 1998, 6259, 2964, 1010, 2021, 8559, 9760, 2987, 1005, 1056, 2428, 2031, 2505, 2000, 2079, 2007, 2122, 2477, 1010, 1998, 2009, 2323, 2022, 2921, 3584, 1012, 1996, 2111, 2027, 10263, 2005, 2023, 3185, 2707, 2007, 1996, 4784, 1997, 8559, 3399, 1998, 2059, 2191, 1996, 11679, 2000, 2360, 2008, 3432, 2011, 3241, 2055, 2242, 2017, 2064, 11477, 1996, 3043, 2105, 2017, 1010, 6516, 2057, 2323, 2228, 13567, 2061, 2004, 2000, 2031, 1037, 3893, 4254, 2006, 1996, 2088, 1998, 2191, 2256, 3268, 2488, 1012, 1996, 13384, 2003, 3294, 9951, 1010, 1998, 1996, 15306, 2079, 2025, 11177, 2135, 3582, 2013, 8559, 3399, 1012, 2005, 2296, 2061, 2170, 1000, 6739, 1000, 2008, 2027, 10263, 2005, 2023, 2143, 1010, 2045, 2024, 7644, 1997, 22634, 13702, 2015, 2040, 2052, 3294, 21090, 1012, 2027, 2052, 2391, 2041, 1010, 3243, 2157, 2135, 1010, 2008, 1996, 4895, 28139, 29201, 8010, 1997, 1996, 4942, 10610, 7712, 2088, 2515, 2025, 18496, 2490, 2000, 17529, 21951, 2055, 2256, 6259, 4198, 2791, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2009, 22995, 2015, 2033, 2008, 2111, 2024, 2183, 2000, 2156, 2023, 2143, 1998, 3294, 4521, 2009, 2039, 2138, 2009, 3727, 2068, 2007, 1037, 3835, 3893, 3110, 1012, 1996, 2364, 7400, 1997, 1996, 2143, 2003, 2241, 2006, 1037, 2561, 28616, 18447, 2121, 28139, 12516, 1997, 8559, 3399, 1010, 1998, 2009, 2003, 2004, 2919, 1999, 2049, 13384, 2004, 2151, 3535, 2000, 16114, 4114, 4676, 2007, 2714, 18404, 1011, 4045, 9918, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 4468, 2023, 2143, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2821, 3398, 1012, 2012, 2028, 2391, 1010, 2028, 1997, 1996, 1000, 8519, 1000, 2758, 2008, 2144, 2802, 2381, 2087, 1997, 1996, 17568, 2111, 2031, 2081, 2055, 1996, 2088, 2357, 2041, 2000, 2022, 6270, 1010, 3568, 1996, 17568, 2057, 2747, 2907, 2055, 1996, 2088, 2024, 2036, 3497, 2000, 2022, 6270, 1012, 9616, 1029, 2008, 6135, 2515, 2025, 3582, 1012, 1998, 2130, 2065, 2009, 2106, 1010, 1045, 2123, 1005, 1056, 2156, 2129, 2008, 7126, 2010, 6685, 1012, 1045, 2812, 1010, 2065, 2010, 4784, 2412, 2150, 2691, 17568, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'label': [0]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the first row of the train set\n",
    "preprocess_function(dataset[\"train\"][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1937ee8-2c67-49c9-a012-c3a71cd9cb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4367f3f880143dba8e02f74b845fbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b97eb5fe734a49a896c8fe80a7ee5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenized dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c70039b-092d-4085-928d-292119343794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc45f87b-acf0-4cc4-a47b-f409fafba73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary columns\n",
    "tokenized_datasets = tokenized_datasets.remove_columns('review')\n",
    "tokenized_datasets = tokenized_datasets.remove_columns('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a3014b3-13c0-491d-b8c1-bd93be449193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de41d32-33c0-43cb-8599-1e3fbeedb891",
   "metadata": {},
   "source": [
    "# Create the Quantization and Configure the Model\n",
    "\n",
    "## 1. Quantization\n",
    "\n",
    "Quantization represents data with fewer bits, making it a useful technique for reducing memory-usage and accelerating inference especially when it comes to large language models (LLMs)\n",
    "\n",
    "Note that, after a model is quantized it isn’t typically further trained for downstream tasks because training can be unstable due to the lower precision of the weights and activations.\n",
    "\n",
    "PEFT methods only add extra trainable parameters, allowing to train a quantized model with a PEFT adapter on top with some special training methodologies!\n",
    "\n",
    "Note further that: Combining quantization with PEFT can be a good strategy for training even the largest models on a single GPU.\n",
    "\n",
    "For example, `QLoRA is a method that quantizes a model to 4-bits and then trains it with LoRA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f15450a1-d9bf-4194-a207-85fff5308365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /opt/anaconda3/lib/python3.12/site-packages (0.42.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from bitsandbytes) (1.12.0)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in /opt/anaconda3/lib/python3.12/site-packages (from scipy->bitsandbytes) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa7820b0-dbe3-410e-a030-7e2e653b28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet bitsandbytes\n",
    "!pip install --quiet --upgrade transformers # Install latest version of transformers\n",
    "!pip install --quiet --upgrade accelerate\n",
    "!pip install --quiet sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abfd2e10-5933-4a6b-9f7d-5fccc92afdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers bitsandbytes accelerate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c659f29-7258-4472-b171-c93cfb2cdc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "0.42.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "print(bnb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "84b3bd16-6879-495f-903e-513ea8618d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "import bitsandbytes\n",
    "\n",
    "# Create the quantize\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type=\"nf4\",  \n",
    "    bnb_4bit_use_double_quant=True,  \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Or torch.float16 if bfloat16 unsupported\n",
    "    bnb_4bit_skip_modules=[\"classifier\", \"pre_classifier\"]  # skip certain modules\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05e3fc08-830e-4dc0-86a4-f1a75fbd6225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "\n",
    "# config = BitsAndBytesConfig(\n",
    "#     # Quantize the model weights to 4-bit precision upon loading, reducing memory usage.\n",
    "#     load_in_4bit=True,  \n",
    "#     # Use the 'Normalized Float 4' (NF4) data type, which uses a normal distribution to encode weights with just 4 bits\n",
    "#     bnb_4bit_quant_type=\"nf4\",  \n",
    "#     # Apply double quantization: first quantize weights to 4-bit, then quantize the quantization constants used for quantizing weights\n",
    "#     bnb_4bit_use_double_quant=True,  \n",
    "#     # Utilize bfloat16 for computation, which takes less memory\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,  \n",
    "#     # Skip quantization for specified modules, which will be trained separately\n",
    "#     llm_int8_skip_modules=[\"classifier\", \"pre_classifier\"]  \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9785f2dd-279f-41ab-b660-4bc530a3e0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dir: /Users/emmanueldanielchonza/Documents/Parameter-Efficient-Fine-tuning-LLMs\n",
      "['.DS_Store', 'requirements.txt', 'peft-BERT.py', 'Solutions', 'PEFT-QLoRA-BERT-Classifier.ipynb', 'README.md', 'peft_env_py311', 'HUGGINGFACE_API_TOKEN.txt', 'transformers', 'mergedQLoRA-Adapter-BERT-BaseModel.py', 'inference.py', '.ipynb_checkpoints', 'data', 'peft_env']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check current working directory\n",
    "print(\"Current dir:\", os.getcwd())\n",
    "\n",
    "# List contents of current directory\n",
    "print(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2eca3f-d40d-4fbb-8e47-2f2469b1b1a9",
   "metadata": {},
   "source": [
    "## 2. Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cbee8ea6-dd95-4396-882e-788539f7c3d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Configure the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_checkpoint,\n\u001b[1;32m      3\u001b[0m                                                            id2label\u001b[38;5;241m=\u001b[39mID2LABEL,\n\u001b[1;32m      4\u001b[0m                                                            label2id\u001b[38;5;241m=\u001b[39mLABEL2ID,\n\u001b[1;32m      5\u001b[0m                                                            num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      6\u001b[0m                                                            quantization_config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:600\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    599\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    601\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    602\u001b[0m     )\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:317\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:4892\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4889\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4892\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mvalidate_environment(\n\u001b[1;32m   4893\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m   4894\u001b[0m         from_tf\u001b[38;5;241m=\u001b[39mfrom_tf,\n\u001b[1;32m   4895\u001b[0m         from_flax\u001b[38;5;241m=\u001b[39mfrom_flax,\n\u001b[1;32m   4896\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   4897\u001b[0m         weights_only\u001b[38;5;241m=\u001b[39mweights_only,\n\u001b[1;32m   4898\u001b[0m     )\n\u001b[1;32m   4899\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   4900\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:88\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.43.1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m         )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[0;31mImportError\u001b[0m: The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1."
     ]
    }
   ],
   "source": [
    "# Configure the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,\n",
    "                                                           id2label=ID2LABEL,\n",
    "                                                           label2id=LABEL2ID,\n",
    "                                                           num_labels=2,\n",
    "                                                           quantization_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6435cc-332c-443a-b131-b168eda4b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0fab12-8cfa-4b12-8179-dc6e5a04b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to print trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d7890c-bf6d-436b-a969-ef9399a8f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Parameters\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b0a43a-784a-49d7-b4d2-407682da07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337e838-6420-435d-b14b-2d1600ece65e",
   "metadata": {},
   "source": [
    "# Train LoRA Using the QLoRA Config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfd265b-50e4-4ab8-b6e7-52cdedf67561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType, replace_lora_weights_loftq\n",
    "\n",
    "# Set up the LoRA configuration for the model\n",
    "config = LoraConfig(\n",
    "    r=8,  # Rank of the LoRA matrices; a smaller rank reduces memory usage but may affect model performance.\n",
    "    lora_alpha=32,  # Scaling factor applied to the LoRA updates; helps control the contribution of the LoRA weights.\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],  # Specify the modules (weight matrix) within the model where LoRA is applied.\n",
    "    lora_dropout=0.05,  # Dropout probability for LoRA layers to prevent overfitting during training.\n",
    "    bias=\"none\",  # Specifies whether to add learnable biases to the LoRA layers.\n",
    "    task_type=TaskType.SEQ_CLS  # Defines the task type, here it's set to sequence classification.\n",
    ")\n",
    "\n",
    "# Apply the LoRA configuration to the model\n",
    "peft_model = get_peft_model(model, config)\n",
    "\n",
    "# Print the number of trainable parameters in the model after applying LoRA\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3b6489-d3fa-4911-8a1f-103de59db406",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a44286-a5fe-4da8-83e8-eb9f991f2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f18015-aa95-43f1-88fa-34a0abc3afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if batch size is 64\n",
    "# if total documents are 8000\n",
    "# total number of steps (batches of data) to complete 1 full epoch is?\n",
    "12000 // 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0845a3-e519-47ee-a38f-8514a8187159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 32 \n",
    "metric_name = \"f1\"\n",
    "\n",
    "# Set up the training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"distilbert-cls-qlorafinetune-runs\",  # Directory where the model checkpoints and outputs will be saved.\n",
    "    eval_strategy=\"steps\",                          # Perform evaluation at regular intervals during training.\n",
    "    save_strategy=\"steps\",                          # Save the model checkpoint at regular intervals.\n",
    "    learning_rate=1e-4,                             # Initial learning rate for the optimizer.\n",
    "    logging_steps=20,                               # Log training metrics every 20 steps.\n",
    "    eval_steps=20,                                  # Perform evaluation every 20 steps.\n",
    "    save_steps=50,                                  # Save the model checkpoint every 50 steps.\n",
    "    per_device_train_batch_size=batch_size,         # Batch size per GPU/TPU core/CPU during training.\n",
    "    per_device_eval_batch_size=batch_size,          # Batch size per GPU/TPU core/CPU during evaluation.\n",
    "    max_steps=250,                                  # Stop training after 250 total steps.\n",
    "    weight_decay=0.01,                              # Apply weight decay to reduce overfitting.\n",
    "    metric_for_best_model=metric_name,              # Metric to use for selecting the best model during evaluation.\n",
    "    push_to_hub=False,                              # Do not push the model to the Hugging Face Hub after training.\n",
    "    fp16=True,                                      # Use 16-bit floating point precision to reduce memory usage and speed up training.\n",
    "    optim=\"paged_adamw_8bit\",                       # Use an 8-bit AdamW optimizer for memory efficiency and faster computation.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016bbe07-c989-41fc-aba0-766809a28f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "# Create the datas collator to padd tokenizs\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c8ab37-1725-47af-a036-12e4f1eb510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create the metric compute\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return evaluate_performance(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca599510-2610-4613-ae9b-04a044e5cde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c39e47-b2dc-4f3b-90e6-0ea3a9424746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780229dd-2725-4f8d-be4a-51231260def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'qlora-bert-sentiment-adapter'\n",
    "trainer.save_model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b2b4d5-80ea-4c0c-9705-c67c81dd1b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove model checkpoints\n",
    "!rm -rf distilbert-cls-qlorafinetune-runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264df789-7685-4a88-8e22-1e764fae90cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh * | sort -hr | grep qlora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077ac73-dc78-41fc-9537-41f4bb8a8b2d",
   "metadata": {},
   "source": [
    "## Load Classification LoRA Adapter into Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74edf00e-e512-40e3-9aa4-6bbd29949e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the base BERT model first\n",
    "cls_model = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-uncased',\n",
    "                                                                id2label=ID2LABEL,\n",
    "                                                                label2id=LABEL2ID,\n",
    "                                                                num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased', fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f8911-dd4c-4bf9-a235-3d5dd709e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model.load_adapter(peft_model_id='qlora-bert-sentiment-adapter',\n",
    "                       adapter_name='sentiment-classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6142d863-28a2-4179-987e-383f15d21465",
   "metadata": {},
   "source": [
    "# Using the Fine-tuned model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46507d52-6fb3-4c20-ae29-4d786f1147f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Here you can load your locally trained \\ saved model\n",
    "clf = pipeline(task='text-classification', \n",
    "               model=cls_model, \n",
    "               tokenizer=tokenizer, \n",
    "               device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad1dd6-6d25-4005-938e-48a81e777f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"The movie was not good at all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a4b6f-e28f-4993-8470-801ae124ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd947c44-9572-4afe-99ce-094cdeada16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = \"The movie was amazing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee9bdb-a277-47ee-9c53-e467514c55eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf(document1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8f397-5acc-49ef-bf37-ff641c13ded5",
   "metadata": {},
   "source": [
    "## Use the Fine-tuned Transformer to Make Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100aeb9-d053-489e-9d2e-1bc3afa88f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286156e-e690-4282-977f-4603e33088e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictions = clf(dataset['test']['review'],\n",
    "                  batch_size=512, \n",
    "                  max_length=512, \n",
    "                  truncation=True)\n",
    "predictions = [pred['label'] for pred in predictions]\n",
    "\n",
    "predictions = [0 if item == 'NEGATIVE' else 1 for item in predictions]\n",
    "labels = dataset['test']['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f5fc4b-e166-41b8-bf44-3f9e425a1373",
   "metadata": {},
   "source": [
    "# Evaluate the Model Performance on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d0b767-0684-4bfc-90f4-d9e42078f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(labels, predictions))\n",
    "pd.DataFrame(confusion_matrix(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47838c5a-41de-40d4-a14a-4c437e3bd875",
   "metadata": {},
   "source": [
    "## Merge Classification LoRA Adapter into Base BERT Model\n",
    "\n",
    "Instead of loading the LoRA model adapter weights into the base model everytime and doing inference, merge the weights directly with the weights of the base model and make a final model. \n",
    "\n",
    "This helps with faster inference loading both model and adapter everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b02ca-c665-496a-9891-14453b626424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the base model and fine-tuned\n",
    "peft_model_id = \"qlora-bert-sentiment-adapter\"\n",
    "config = PeftConfig.from_pretrained(save_path) # peft_model_id or save_path\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path,\n",
    "                                                                id2label=ID2LABEL,\n",
    "                                                                label2id=LABEL2ID,\n",
    "                                                                num_labels=2)\n",
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, fast=True)\n",
    "\n",
    "# Load the model to device\n",
    "peft_model = PeftModel.from_pretrained(base_model, save_path).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e357219-396f-47df-8540-cfe2435cc500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the models\n",
    "merged_cls_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8796411b-0067-4c4d-8bd4-27c4fc7b6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_path = 'merged-qlora-bert-classifier'\n",
    "\n",
    "merged_cls_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7abea2-33f9-483d-8447-1f842c85815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the merged BERT model \n",
    "cls_model = AutoModelForSequenceClassification.from_pretrained('merged-qlora-bert-classifier',\n",
    "                                                                id2label=ID2LABEL,\n",
    "                                                                label2id=LABEL2ID,\n",
    "                                                                num_labels=2)\n",
    "\n",
    "# Create tokenizer using the merged model\n",
    "tokenizer = AutoTokenizer.from_pretrained('merged-qlora-bert-classifier', fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d8df0-e3d1-4bf1-bb5d-5bfa7465a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier\n",
    "clf = pipeline(task='text-classification', \n",
    "               model=cls_model, \n",
    "               tokenizer=tokenizer, \n",
    "               device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc756f-731f-45b3-a8e5-818cdae7bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"The movie was not good at all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df02cf73-8049-414f-8c93-72146916ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206125ac-a439-4697-83fc-7b927c7b4c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "document2 = \"The movie was amazing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d69bd92-e912-4b97-8fcc-7d9c14f13b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf(document2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
